name: '[Monitor] BYOVD & EDR Evasion Projects & Updates'
on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  discover:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }}

      - name: Setup GitHub CLI and jq
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
          sudo apt update
          sudo apt install gh -y

      - name: Authenticate GitHub CLI
        run: echo "${{ secrets.GH_PAT }}" | gh auth login --with-token

      - name: Fetch latest project updates
        run: |
          QUERY="%22BYOVD%22%20AND%20(kill%20OR%20antiav%20OR%20antiedr)%20in:name,description,readme"
          API_URL="https://api.github.com/search/repositories?q=$QUERY&sort=updated&order=desc&per_page=30"
          echo "è°ƒç”¨API: $API_URL"
          gh api "$API_URL" \
            --jq '.items[] | {full_name, html_url, description, stargazers_count, updated_at, pushed_at, created_at, topics }' \
            > latest_results.json
          jq -r '.html_url' latest_results.json > current_urls.txt
        env:
          GH_FORCE_TTY: 0

      - name: Retrieve previous state
        run: |
          if [ -f previous_results.json ]; then
            cp previous_results.json previous_results_backup.json
            jq -r '.html_url' previous_results.json > previous_urls.txt
          else
            touch previous_urls.txt
            echo "[]" > previous_results_backup.json
          fi

      - name: Identify new and updated projects
        id: check_changes
        run: |
          grep -Fxv -f previous_urls.txt current_urls.txt > new_items.txt || true
          NEW_COUNT=$(wc -l < new_items.txt | tr -d ' ')
          echo "å‘ç° $NEW_COUNT ä¸ªæ–°é¡¹ç›®"

          # åˆ›å»ºPythonè„šæœ¬æ¥æ£€æµ‹æ›´æ–°
          cat > detect_updates.py << 'EOF'
import json
from datetime import datetime, timedelta
import os

# åŠ è½½å½“å‰å’Œä¹‹å‰çš„ç»“æœ
current_items = []
with open('latest_results.json', 'r') as f:
    for line in f:
        if line.strip():
            current_items.append(json.loads(line))

previous_items = []
if os.path.exists('previous_results_backup.json'):
    with open('previous_results_backup.json', 'r') as f:
        for line in f:
            if line.strip():
                previous_items.append(json.loads(line))

# åˆ›å»ºæ˜ å°„ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
previous_map = {item['html_url']: item for item in previous_items if 'html_url' in item}

updated_count = 0
updated_urls = []

for current in current_items:
    url = current.get('html_url')
    if not url:
        continue
        
    previous = previous_map.get(url)
    
    if previous:
        try:
            # è§£ææ—¶é—´å­—ç¬¦ä¸²
            current_updated = datetime.fromisoformat(current['updated_at'].replace('Z', '+00:00'))
            previous_updated = datetime.fromisoformat(previous['updated_at'].replace('Z', '+00:00'))
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æœ€è¿‘ä¸€æ¬¡æ£€æŸ¥ä¹‹åæœ‰æ›´æ–°
            if current_updated > previous_updated + timedelta(hours=1):
                # æ£€æŸ¥å…¶ä»–é‡è¦å˜åŒ–
                star_increase = current.get('stargazers_count', 0) - previous.get('stargazers_count', 0)
                current_topics = set(current.get('topics', []))
                previous_topics = set(previous.get('topics', []))
                new_topics = current_topics - previous_topics
                
                # å¦‚æœæœ‰æ˜¾è‘—æ›´æ–°ï¼Œåˆ™æ ‡è®°
                if (current_updated - previous_updated).days < 7 or star_increase > 2 or new_topics:
                    updated_count += 1
                    updated_urls.append(url)
                    print(f"æ£€æµ‹åˆ°æ›´æ–°: {url}")
                    
        except (KeyError, ValueError) as e:
            print(f"å¤„ç†é¡¹ç›® {url} æ—¶å‡ºé”™: {e}")
            continue

# ä¿å­˜æ›´æ–°åˆ—è¡¨
with open('updated_items.txt', 'w') as f:
    for url in updated_urls:
        f.write(f"{url}\n")

print(f"å‘ç° {updated_count} ä¸ªé¡¹ç›®æœ‰æ›´æ–°")
EOF

          # æ‰§è¡ŒPythonè„šæœ¬
          python3 detect_updates.py

          UPDATED_COUNT=$(wc -l < updated_items.txt | tr -d ' ')
          echo "å‘ç° $UPDATED_COUNT ä¸ªé¡¹ç›®æœ‰é‡è¦æ›´æ–°"

          TOTAL_CHANGES=$((NEW_COUNT + UPDATED_COUNT))
          if [ $TOTAL_CHANGES -gt 0 ]; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "CHANGE_COUNT=$TOTAL_CHANGES" >> $GITHUB_ENV
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "CHANGE_COUNT=0" >> $GITHUB_ENV
          fi

      - name: Generate RSS feed for changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          CURRENT_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          RSS_ITEM_FILE="rss_items.xml"

          if [ -s new_items.txt ]; then
            jq --slurpfile new_urls new_items.txt -r --arg date "$CURRENT_DATE" '
              .[] | select(.html_url as $u | $new_urls[] | index($u)) |
              @html "  <item>\n    <title>ğŸš€ æ–°é¡¹ç›®: \(.full_name)</title>\n    <link>\(.html_url)</link>\n    <description>æ–°å‘ç°çš„BYOVD/EDRå¯¹æŠ—é¡¹ç›®ã€‚æè¿°: \(.description // \"No description\" | @html)</description>\n    <pubDate>\($date)</pubDate>\n    <guid isPermaLink=\"true\">\(.html_url)?new=1</guid>\n  </item>"
            ' latest_results.json > $RSS_ITEM_FILE
          fi

          if [ -s updated_items.txt ]; then
            jq --slurpfile updated_urls updated_items.txt -r --arg date "$CURRENT_DATE" '
              .[] | select(.html_url as $u | $updated_urls[] | index($u)) |
              @html "  <item>\n    <title>ğŸ“¢ æ›´æ–°: \(.full_name)</title>\n    <link>\(.html_url)</link>\n    <description>é¡¹ç›®æœ‰é‡è¦æ›´æ–°ï¼Œå¯èƒ½å‘å¸ƒäº†æ–°ç‰ˆæœ¬æˆ–æ·»åŠ äº†æ–°åŠŸèƒ½ã€‚æè¿°: \(.description // \"No description\" | @html)</description>\n    <pubDate>\($date)</pubDate>\n    <guid isPermaLink=\"true\">\(.html_url)?updated=\(now|floor)</guid>\n  </item>"
            ' latest_results.json >> $RSS_ITEM_FILE
          fi

      - name: Update RSS feed and persist state
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          # åˆ›å»ºPythonè„šæœ¬æ¥æ›´æ–°RSS
          cat > update_rss.py << 'EOF'
import xml.etree.ElementTree as ET
from xml.dom import minidom
import datetime
import os

# è§£æç°æœ‰çš„RSS feed
rss_file = 'feed.xml'
if os.path.exists(rss_file):
    tree = ET.parse(rss_file)
    root = tree.getroot()
    channel = root.find('channel')
else:
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
    root = ET.Element('rss')
    root.set('version', '2.0')
    channel = ET.SubElement(root, 'channel')
    title = ET.SubElement(channel, 'title')
    title.text = 'GitHub Monitor: BYOVD & EDR Evasion'
    link = ET.SubElement(channel, 'link')
    link.text = f'https://github.com/{os.environ.get("GITHUB_REPOSITORY", "")}'
    description = ET.SubElement(channel, 'description')
    description.text = 'ç›‘æ§BYOVDåŠEDRå¯¹æŠ—é¡¹ç›®çš„æ›´æ–°å’Œæ–°å‡ºç°'
    language = ET.SubElement(channel, 'language')
    language.text = 'en-us'

# è§£ææ–°ç”Ÿæˆçš„items
rss_item_file = 'rss_items.xml'
if os.path.exists(rss_item_file):
    try:
        item_tree = ET.parse(rss_item_file)
        for item in reversed(item_tree.findall('item')):
            channel.insert(4, item)  # åœ¨lastBuildDateä¹‹åæ’å…¥
    except:
        pass

# æ›´æ–°lastBuildDate
last_build_date = channel.find('lastBuildDate')
if last_build_date is None:
    last_build_date = ET.SubElement(channel, 'lastBuildDate')
last_build_date.text = datetime.datetime.now(datetime.timezone.utc).strftime('%a, %d %b %Y %H:%M:%S GMT')

# ä¿å­˜æ–‡ä»¶
rough_string = ET.tostring(root, encoding='unicode')
parsed = minidom.parseString(rough_string)
with open(rss_file, 'w', encoding='utf-8') as f:
    f.write(parsed.toprettyxml(indent='  '))
EOF

          python3 update_rss.py

          # ä¿å­˜å½“å‰å®Œæ•´çŠ¶æ€ï¼Œç”¨äºä¸‹æ¬¡æ¯”è¾ƒ
          cp latest_results.json previous_results.json
          cp current_urls.txt previous_urls.txt

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add feed.xml previous_results.json previous_urls.txt
          git commit -m "CI: Found ${{ env.CHANGE_COUNT }} changes ($NEW_COUNT new, $UPDATED_COUNT updated)"
          git push

      - name: No changes notification
        if: steps.check_changes.outputs.has_changes == 'false'
        run: |
          date +"%Y-%m-%d %H:%M:%S - No changes detected" >> monitor.log
          git add monitor.log
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git commit -m "CI: Monitoring completed, no changes" || exit 0
          git push || exit 0
